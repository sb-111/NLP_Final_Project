{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d6714cf45d5e661"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim.models.fasttext import FastText, load_facebook_vectors, load_facebook_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "def preprocess_korean_text(text, stop_words=[]):\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 형태소 분석을 통한 토큰화\n",
    "    words = okt.morphs(text, norm=True) # 정규화 처리\n",
    "    \n",
    "    # 불용어 제거\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def process_csv_file(csv_path, stop_words=[]):\n",
    "    new_data = []\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    if 'reviews' in df.columns:\n",
    "        for text in df['reviews']:\n",
    "            # 추가적인 전처리 작업 (예: 특수 문자 제거, 숫자 제거 등)\n",
    "            text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # 전처리된 문장을 추가 데이터에 포함\n",
    "            tokens = preprocess_korean_text(text, stop_words)\n",
    "            new_data.append(tokens)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 기존 pre-trained 모델 경로\n",
    "pretrained_model_path = 'wiki.ko.bin'\n",
    "\n",
    "# 새로운 데이터 파일 경로\n",
    "csv_file_path = '../ReviewData/reviews_only.csv'\n",
    "\n",
    "# 불용어 리스트 (필요에 따라 추가)\n",
    "stop_words = ['의','가', '에','들','는','잘','걍','과','도','를','으로','한','하다','!','?','<','>','(',')','[',']','|','#','.', '이','은','는','을','에','에서','로']\n",
    "\n",
    "# 모델 로드\n",
    "# model = load_facebook_vectors(pretrained_model_path)\n",
    "model = load_facebook_model(pretrained_model_path)\n",
    "\n",
    "# CSV 파일 처리 및 전처리\n",
    "new_data = process_csv_file(csv_file_path, stop_words)\n",
    "\n",
    "# 모델 업데이트\n",
    "model.build_vocab(corpus_iterable=new_data, update=True)\n",
    "model.train(corpus_iterable=new_data, total_examples=len(new_data), epochs=model.epochs)\n",
    "\n",
    "# 업데이트된 모델 저장\n",
    "updated_model_path = 'updated_model.bin'\n",
    "model.save(updated_model_path)\n",
    "\n",
    "# 저장된 모델 경로 출력\n",
    "print(f\"모델이 성공적으로 업데이트되었습니다. 업데이트된 모델 경로: {os.path.abspath(updated_model_path)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T06:54:38.437516Z"
    }
   },
   "id": "6ffa3865585819a3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력한 단어 '치킨'와(과) 유사한 단어들:\n",
      "양념치킨: 0.8387115001678467\n",
      "치킨과: 0.8261151909828186\n",
      "치킨윙: 0.8219879865646362\n",
      "치킨에: 0.819800078868866\n",
      "치킨이: 0.809363842010498\n",
      "치킨은: 0.7859618663787842\n",
      "치킨을: 0.785783588886261\n",
      "치킨의: 0.777712881565094\n",
      "먼치킨: 0.7542405724525452\n",
      "치킨집: 0.7345045804977417\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# 저장한 모델 불러오기\n",
    "model = FastText.load('./updated_model.bin')\n",
    "\n",
    "def find_similar_words(word, topn=10):\n",
    "    try:\n",
    "        # 입력한 단어와 유사한 단어들을 찾기\n",
    "        similar_words = model.wv.most_similar(word, topn=topn)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"입력한 단어 '{word}'와(과) 유사한 단어들:\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"{similar_word}: {similarity}\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"입력한 단어 '{word}'는 모델의 어휘에 없습니다.\")\n",
    "\n",
    "# 사용자로부터 단어 입력 받기\n",
    "user_input = input(\"단어를 입력하세요: \")\n",
    "\n",
    "# 유사한 단어 찾기 및 출력\n",
    "find_similar_words(user_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T06:56:58.178071Z",
     "start_time": "2023-12-08T06:55:44.002246Z"
    }
   },
   "id": "7d499cf0883628ac"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 성공적으로 업데이트되었습니다. 업데이트된 모델 경로: E:\\NLP_PROJECT\\NLP_Final_Project\\Preprocess\\updated_model.bin\n"
     ]
    }
   ],
   "source": [
    "# 조금 변형해본 코드\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models.fasttext import FastText, load_facebook_vectors, load_facebook_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "def preprocess_korean_text(text, stop_words=[], pos_tags=['Noun']):\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 형태소 분석을 통한 토큰화 및 품사 태깅\n",
    "    morphs = okt.pos(text, norm=True, stem=True)\n",
    "    \n",
    "    # 선택한 품사의 단어만 추출\n",
    "    words = [word for word, pos in morphs if pos in pos_tags]\n",
    "    \n",
    "    # 불용어 제거\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def process_csv_file(csv_path, stop_words=[]):\n",
    "    new_data = []\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    if 'reviews' in df.columns:\n",
    "        for text in df['reviews']:\n",
    "            # 추가적인 전처리 작업 (예: 특수 문자 제거, 숫자 제거 등)\n",
    "            text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # 전처리된 문장을 추가 데이터에 포함\n",
    "            tokens = preprocess_korean_text(text, stop_words)\n",
    "            new_data.append(tokens)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 기존 pre-trained 모델 경로\n",
    "pretrained_model_path = 'wiki.ko.bin'\n",
    "\n",
    "# 새로운 데이터 파일 경로\n",
    "csv_file_path = '../ReviewData/reviews_only.csv'\n",
    "\n",
    "# 불용어 리스트 (필요에 따라 추가)\n",
    "stop_words = ['의','가', '에','들','는','잘','걍','과','도','를','으로','한','하다','!','?','<','>','(',')','[',']','|','#','.', '이','은','는','을','에','에서','로']\n",
    "\n",
    "# 모델 로드\n",
    "# model = load_facebook_vectors(pretrained_model_path)\n",
    "model = load_facebook_model(pretrained_model_path)\n",
    "\n",
    "# CSV 파일 처리 및 전처리\n",
    "new_data = process_csv_file(csv_file_path, stop_words)\n",
    "\n",
    "# 모델 업데이트\n",
    "model.build_vocab(corpus_iterable=new_data, update=True)\n",
    "model.train(corpus_iterable=new_data, total_examples=len(new_data), epochs=model.epochs)\n",
    "\n",
    "# 업데이트된 모델 저장\n",
    "updated_model_path = 'updated_model.bin'\n",
    "model.save(updated_model_path)\n",
    "\n",
    "# 저장된 모델 경로 출력\n",
    "print(f\"모델이 성공적으로 업데이트되었습니다. 업데이트된 모델 경로: {os.path.abspath(updated_model_path)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T07:27:23.098918900Z",
     "start_time": "2023-12-08T07:25:08.061875800Z"
    }
   },
   "id": "8abadf88855f5ec7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfasttext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FastText\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 저장한 모델 불러오기\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mFastText\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./updated_model.bin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind_similar_words\u001B[39m(word, topn\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;66;03m# 입력한 단어와 유사한 단어들을 찾기\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\fasttext.py:637\u001B[0m, in \u001B[0;36mFastText.load\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    617\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    619\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a previously saved `FastText` model.\u001B[39;00m\n\u001B[0;32m    620\u001B[0m \n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    635\u001B[0m \n\u001B[0;32m    636\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(FastText, \u001B[38;5;28mcls\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;241m*\u001B[39margs, rethrow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1942\u001B[0m, in \u001B[0;36mWord2Vec.load\u001B[1;34m(cls, rethrow, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \n\u001B[0;32m   1925\u001B[0m \u001B[38;5;124;03mSee Also\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1939\u001B[0m \n\u001B[0;32m   1940\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1941\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1942\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(Word2Vec, \u001B[38;5;28mcls\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1943\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, Word2Vec):\n\u001B[0;32m   1944\u001B[0m         rethrow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\utils.py:487\u001B[0m, in \u001B[0;36mSaveLoad.load\u001B[1;34m(cls, fname, mmap)\u001B[0m\n\u001B[0;32m    484\u001B[0m compress, subname \u001B[38;5;241m=\u001B[39m SaveLoad\u001B[38;5;241m.\u001B[39m_adapt_by_suffix(fname)\n\u001B[0;32m    486\u001B[0m obj \u001B[38;5;241m=\u001B[39m unpickle(fname)\n\u001B[1;32m--> 487\u001B[0m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_specials\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmmap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    488\u001B[0m obj\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloaded\u001B[39m\u001B[38;5;124m\"\u001B[39m, fname\u001B[38;5;241m=\u001B[39mfname)\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\fasttext.py:641\u001B[0m, in \u001B[0;36mFastText._load_specials\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_specials\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    640\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 641\u001B[0m     \u001B[38;5;28msuper\u001B[39m(FastText, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m_load_specials(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbucket\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    643\u001B[0m         \u001B[38;5;66;03m# should only exist in one place: the wv subcomponent\u001B[39;00m\n\u001B[0;32m    644\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mbucket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbucket\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1958\u001B[0m, in \u001B[0;36mWord2Vec._load_specials\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1956\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_specials\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1957\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1958\u001B[0m     \u001B[38;5;28msuper\u001B[39m(Word2Vec, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m_load_specials(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1959\u001B[0m     \u001B[38;5;66;03m# for backward compatibility, add/rearrange properties from prior versions\u001B[39;00m\n\u001B[0;32m   1960\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mns_exponent\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\utils.py:518\u001B[0m, in \u001B[0;36mSaveLoad._load_specials\u001B[1;34m(self, fname, mmap, compress, subname)\u001B[0m\n\u001B[0;32m    516\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m recursively from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.* with mmap=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, attrib, cfname, mmap)\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ignore_deprecation_warning():\n\u001B[1;32m--> 518\u001B[0m         \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrib\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_specials\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmmap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attrib \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__numpys\u001B[39m\u001B[38;5;124m'\u001B[39m, []):\n\u001B[0;32m    521\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m with mmap=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, attrib, subname(fname, attrib), mmap)\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\fasttext.py:1005\u001B[0m, in \u001B[0;36mFastTextKeyedVectors._load_specials\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_specials\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1004\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1005\u001B[0m     \u001B[38;5;28msuper\u001B[39m(FastTextKeyedVectors, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m_load_specials(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1006\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, FastTextKeyedVectors):\n\u001B[0;32m   1007\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoaded object of type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, not expected FastTextKeyedVectors\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:263\u001B[0m, in \u001B[0;36mKeyedVectors._load_specials\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_specials\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    262\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 263\u001B[0m     \u001B[38;5;28msuper\u001B[39m(KeyedVectors, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m_load_specials(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdoctags\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upconvert_old_d2vkv()\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\utils.py:529\u001B[0m, in \u001B[0;36mSaveLoad._load_specials\u001B[1;34m(self, fname, mmap, compress, subname)\u001B[0m\n\u001B[0;32m    527\u001B[0m     val \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(subname(fname, attrib))[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 529\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubname\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrib\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmmap_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmmap\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ignore_deprecation_warning():\n\u001B[0;32m    532\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attrib, val)\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\numpy\\lib\\npyio.py:456\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    453\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m.\u001B[39mopen_memmap(file, mode\u001B[38;5;241m=\u001B[39mmmap_mode,\n\u001B[0;32m    454\u001B[0m                                   max_header_size\u001B[38;5;241m=\u001B[39mmax_header_size)\n\u001B[0;32m    455\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 456\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_pickle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mpickle_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpickle_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mmax_header_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_header_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;66;03m# Try a pickle\u001B[39;00m\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_pickle:\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\numpy\\lib\\format.py:809\u001B[0m, in \u001B[0;36mread_array\u001B[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001B[0m\n\u001B[0;32m    806\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m isfileobj(fp):\n\u001B[0;32m    808\u001B[0m         \u001B[38;5;66;03m# We can use the fast fromfile() function.\u001B[39;00m\n\u001B[1;32m--> 809\u001B[0m         array \u001B[38;5;241m=\u001B[39m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    810\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    811\u001B[0m         \u001B[38;5;66;03m# This is not a real file. We have to read it the\u001B[39;00m\n\u001B[0;32m    812\u001B[0m         \u001B[38;5;66;03m# memory-intensive way.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001B[39;00m\n\u001B[0;32m    822\u001B[0m         array \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mndarray(count, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# 저장한 모델 불러오기\n",
    "model = FastText.load('./updated_model.bin')\n",
    "\n",
    "def find_similar_words(word, topn=10):\n",
    "    try:\n",
    "        # 입력한 단어와 유사한 단어들을 찾기\n",
    "        similar_words = model.wv.most_similar(word, topn=topn)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"입력한 단어 '{word}'와(과) 유사한 단어들:\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"{similar_word}: {similarity}\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"입력한 단어 '{word}'는 모델의 어휘에 없습니다.\")\n",
    "\n",
    "# 사용자로부터 단어 입력 받기\n",
    "user_input = input(\"단어를 입력하세요: \")\n",
    "\n",
    "# 유사한 단어 찾기 및 출력\n",
    "find_similar_words(user_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:15:23.521009200Z",
     "start_time": "2023-12-11T09:15:13.957659100Z"
    }
   },
   "id": "aeb0a8c1c5b9f44a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter corpus_file must be a valid path to a file, got 'reviewdata' instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 47\u001B[0m\n\u001B[0;32m     44\u001B[0m new_data \u001B[38;5;241m=\u001B[39m process_csv_file(csv_file_path, stop_words)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# 모델 학습\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimple_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_file_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 이 부분 수정\u001B[39;00m\n\u001B[0;32m     48\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(corpus_iterable\u001B[38;5;241m=\u001B[39mnew_data, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(new_data), epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# 모델 저장\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\word2vec.py:490\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_vocab\u001B[39m(\n\u001B[0;32m    450\u001B[0m         \u001B[38;5;28mself\u001B[39m, corpus_iterable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, corpus_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, progress_per\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[0;32m    451\u001B[0m         keep_raw_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, trim_rule\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    452\u001B[0m     ):\n\u001B[0;32m    453\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \n\u001B[0;32m    455\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    488\u001B[0m \n\u001B[0;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 490\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_corpus_sanity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    491\u001B[0m     total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_vocab(\n\u001B[0;32m    492\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, progress_per\u001B[38;5;241m=\u001B[39mprogress_per, trim_rule\u001B[38;5;241m=\u001B[39mtrim_rule)\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n",
      "File \u001B[1;32mE:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1501\u001B[0m, in \u001B[0;36mWord2Vec._check_corpus_sanity\u001B[1;34m(self, corpus_iterable, corpus_file, passes)\u001B[0m\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoth corpus_file and corpus_iterable must not be provided at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(corpus_file):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter corpus_file must be a valid path to a file, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m corpus_file)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(corpus_iterable, Iterable):\n\u001B[0;32m   1503\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   1504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe corpus_iterable must be an iterable of lists of strings, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m corpus_iterable)\n",
      "\u001B[1;31mTypeError\u001B[0m: Parameter corpus_file must be a valid path to a file, got 'reviewdata' instead"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim.models.fasttext import FastText\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def preprocess_korean_text(text, stop_words=[]):\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 형태소 분석을 통한 토큰화\n",
    "    morphs = okt.morphs(text, norm=True, stem=True)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    words = [word for word in morphs if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def process_csv_file(csv_path, stop_words=[]):\n",
    "    new_data = []\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    if 'reviews' in df.columns:\n",
    "        for text in df['reviews']:\n",
    "            # 추가적인 전처리 작업 (예: 특수 문자 제거, 숫자 제거 등)\n",
    "            text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # 전처리된 문장을 추가 데이터에 포함\n",
    "            tokens = preprocess_korean_text(text, stop_words)\n",
    "            new_data.append(tokens)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# 새로운 데이터 파일 경로\n",
    "csv_file_path = '../ReviewData/reviews_only.csv'\n",
    "\n",
    "# 불용어 리스트 (필요에 따라 추가)\n",
    "stop_words = ['의','가', '에','들','는','잘','걍','과','도','를','으로','한','하다','!','?','<','>','(',')','[',']','|','#','.', '이','은','는','을','에','에서','로']\n",
    "\n",
    "# 모델 초기화\n",
    "model = FastText(vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "\n",
    "# CSV 파일 처리 및 전처리\n",
    "new_data = process_csv_file(csv_file_path, stop_words)\n",
    "\n",
    "# 모델 학습\n",
    "model.build_vocab(corpus_file=simple_preprocess(csv_file_path)[0])  # 이 부분 수정\n",
    "model.train(corpus_iterable=new_data, total_examples=len(new_data), epochs=10)\n",
    "\n",
    "# 모델 저장\n",
    "model_path = 'new_model.bin'\n",
    "model.save(model_path)\n",
    "\n",
    "# 저장된 모델 경로 출력\n",
    "print(f\"모델이 성공적으로 생성되었습니다. 모델 경로: {os.path.abspath(model_path)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T07:33:26.457367600Z",
     "start_time": "2023-12-08T07:33:25.742629400Z"
    }
   },
   "id": "9eee65dd9d320b80"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3_envs\\nlp_venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      " ['너어어어어어무 맛있게 잘 먹었어요ㅋㅋ', '잘먹었어요', '맛있게 잘 먹었어요^^', '정말 맛있게 잘 먹었어요ㅋㅋ', '맛있게 잘먹었어요 ㅎ !!!']\n",
      "Cluster 2:\n",
      " ['맛있게 잘먹었습니다', '맛있게 잘 먹었습니다.', '맛있게 잘 먹었습니다', '맛있게 잘먹었습니다', '맛있게 잘 먹었습니다']\n",
      "Cluster 3:\n",
      " ['잡채밥은 첨시켰는데 맛있어요!', '너무 맛있어요！！！！！！！！', '맛있어요. 배달 도 빨라요', '맛있어요맛있어요맛있어요', '십분만에 왔어요 맛있어요']\n",
      "Cluster 4:\n",
      " ['배달 초스피드 개맛있음', '배달도 빠르고 가격도 착하고 맛도 굿', '배달도 빠르고 양이 엄청 많아여', '배달 빠르고 맛있어요', '항상 배달도 빠르고 맛있어요 잘먹었습니다']\n",
      "Cluster 5:\n",
      " ['오랜만에  완두콩,옥수수 들어있는 짜장면을 먹어보네요!배달도 무척이나 빠르고짬뽕국물도 서비스로 오니 야식 시킬때 정말 딱입니다어 근데  제입맛엔  조금 짰어요', '맛과 양까지 다 최고예요', '아니 제가 도착하면 문자해달라고 체크해놧는데한시간 넘게 지나서도 안오길레 문자가 혹시나 해서문밖에 보니깐 음식 와있네요^^이미 음식은 추워서 다 식엇고 차가운상태네요문자하나 써주는게 뭐가 어렵다고 참나다신 안시켜먹을듯 하네요', '탕수육은 맛있었는데 짬뽕, 짜장은 제 입맛에는 좀짜게 느껴지네요.담엔 싱겁게 요청드려야겠네요', '1인분배달되는곳중에 제일 맛있어서 여기서만 시키려구요']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = '../ReviewData/reviews_converted.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# 형태소 분석을 위한 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=okt.morphs, max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# K-means 군집화\n",
    "num_clusters = 5  # 군집 수는 적절하게 조절하세요.\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# 각 리뷰의 군집 할당 결과\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# 결과를 DataFrame에 추가\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# 군집 결과 출력\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_data = df[df['cluster'] == cluster_id]['reviews'].head(5).tolist()\n",
    "    print(f\"Cluster {cluster_id + 1}:\\n\", cluster_data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T08:47:57.654388700Z",
     "start_time": "2023-12-08T08:46:49.072860700Z"
    }
   },
   "id": "beeb71c15593edef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "fasttext.sav"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f888e44c90a05ed6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
